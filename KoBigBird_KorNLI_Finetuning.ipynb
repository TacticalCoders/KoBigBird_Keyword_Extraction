{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBigBird KorNLI fine-tuning\n",
    "\n",
    "긴 문장(최대 4096 토큰)이 입력 가능한 KoBigBird를 두 문장의 entail, contradiction, neutral을 판별할 수 있도록 fine - tuning 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋(KorNLI Datasets)\n",
    "\n",
    "카카오브레인에서 공개한 문장 유사도 데이터셋 KorNLI를 사용합니다.\n",
    "\n",
    "\n",
    "다운로드 링크:\n",
    "https://github.com/kakaobrain/KorNLUDatasets/tree/master/KorNLI\n",
    "\n",
    "KorSTS 데이터셋은 두 문장 쌍과 두 문장의 관계를 entailment, contradiction, neutral 세 개의 클래스 중 하나의 label이 부착된 데이터셋입니다.\n",
    "\n",
    "BigBird에 두 문장 쌍을 입력하여 얻은 벡터 표현과 두 벡터의 차의 절대값을 concat하고 softmax를 거쳐 최종 분류를 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/KorNLI/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, dev, test data 읽어오기\n",
    "\n",
    "리스트의 각 원소는 리스트로 저장됩니다. [[문장1, 문장2], gold_label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "train_mnli_data = []\n",
    "train_snli_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "\n",
    "with open(data_path + 'multinli.train.ko.tsv') as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in tsv_file:\n",
    "        train_mnli_data.append([[line[0], line[1]], line[2]])\n",
    "        \n",
    "with open(data_path + 'snli_1.0_train.ko.tsv') as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in tsv_file:\n",
    "        train_snli_data.append([[line[0], line[1]], line[2]])\n",
    "        \n",
    "with open(data_path + 'xnli.dev.ko.tsv') as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in tsv_file:\n",
    "        dev_data.append([[line[0], line[1]], line[2]])\n",
    "        \n",
    "with open(data_path + 'xnli.test.ko.tsv') as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in tsv_file:\n",
    "        test_data.append([[line[0], line[1]], line[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnli_data = train_mnli_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snli_data = train_snli_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = dev_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_mnli_data + train_snli_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data 수: 942854\n",
      "dev set 수: 2490\n",
      "test set 수: 5010\n"
     ]
    }
   ],
   "source": [
    "print('train_data 수:', len(train_data))\n",
    "print('dev set 수:', len(dev_data))\n",
    "print('test set 수:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label Index 할당하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {'entailment':0, 'contradiction':1, 'neutral':2}\n",
    "idx_to_label = {0:'entailment', 1:'contradiction', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user03/anaconda3/envs/koelectra/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda를 사용합니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"{device}를 사용합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBigBird 모델, 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'bert.pooler.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AutoModel,AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"monologg/kobigbird-bert-base\", attention_type=\"original_full\", add_pooling_layer=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "valid_bath_size = 16\n",
    "epochs = 50\n",
    "learning_rate=5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "데이터셋 클래스에 토크나이저를 받아와서 데이터셋 객체를 생성할 때 토크나이징 하여 리스트로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class stsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.sent1 = []\n",
    "        self.sent2 = []\n",
    "        self.label = []\n",
    "        self.tokenizer = tokenizer\n",
    "        for tup in data:\n",
    "            self.sent1.append(self.tokenizer(tup[0][0], max_length=256, padding='max_length',return_tensors='pt'))\n",
    "            self.sent2.append(self.tokenizer(tup[0][1], max_length=256, padding='max_length',return_tensors='pt'))\n",
    "            self.label.append(label_to_idx[tup[1]])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.label))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sent1[idx], self.sent2[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = stsDataset(train_data, tokenizer)\n",
    "valid_dataset = stsDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "모델 훈련 시에 지정한 배치 크기만큼 데이터를 반환해주는 DataLodader입니다.\n",
    "\n",
    "DataLoader는 Dataset객체를 인자로 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=train_batch_size\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=valid_bath_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련에 필요한 optimizer, 코사인 유사도 함수 손실함수를 정의합니다.\n",
    "\n",
    "optimizer에는 모델의 파라미터를 넘겨주고 학습률을 지정해줘야 합니다.\n",
    "\n",
    "다중 분류 모델이므로 CrossEntropyLoss를 사용합니다.\n",
    "\n",
    "pytorch에서는 CrossEntropyLoss를 사용할 때 label을 one-hot 벡터로 변환해주지 않아도 됩니다. 또한 softmax가 포함되어 있기 때문에 그냥 모델의 logit을 전달하면 됩니다. label로는 위에서 딕셔너리로 정의한 정답 index를 전달하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류층을 추가한 custom 모델 클래스 정의\n",
    "\n",
    "두 문장 각각 벡터와 두 벡터의 차이(element-wise difference)를 concat하여 클래스를 분류합니다. \n",
    "(sbert 논문 참고  https://arxiv.org/pdf/1908.10084.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NLImodel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.kobigbird = model\n",
    "        self.classification_layer = nn.Linear(768 * 3, 3)\n",
    "        \n",
    "    def forward(self, b_sent1, b_sent2):\n",
    "        sent1_vec = self.kobigbird(b_sent1['input_ids'].squeeze(1), b_sent1['attention_mask'].squeeze(1), b_sent1['token_type_ids'].squeeze(1))[0][:,0,:]\n",
    "        sent2_vec = self.kobigbird(b_sent2['input_ids'].squeeze(1), b_sent2['attention_mask'].squeeze(1), b_sent2['token_type_ids'].squeeze(1))[0][:,0,:]\n",
    "        \n",
    "        difference = torch.sub(sent1_vec, sent2_vec)\n",
    "        abs_difference = torch.abs(difference)\n",
    "       \n",
    "        concatenation = torch.cat((sent1_vec, sent2_vec, abs_difference), 1)\n",
    "        pred = self.classification_layer(concatenation)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_model = NLImodel(model)\n",
    "nli_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "optimizer = torch.optim.Adam(nli_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 및 검증 \n",
    "\n",
    "최저 valid loss를 갱신할 때마다 모델을 저장합니다. 현재 디렉토리의 model 폴더에 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    step = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_sent1, b_sent2, b_label = batch \n",
    "        \n",
    "        logits = nli_model(b_sent1, b_sent2)\n",
    "        \n",
    "        loss = loss_fn(logits, b_label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        step += 1\n",
    "        \n",
    "    print(f\"train {epoch} epoch 종료  평균 loss: {sum(train_loss)/step}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = []\n",
    "        step = 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(valid_dataloader)):\n",
    "            \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_sent1, b_sent2, b_label = batch # [batch_size, max_len, 768]\n",
    "\n",
    "            logits = nli_model(b_sent1, b_sent2)\n",
    "            \n",
    "            loss = loss_fn(logits, b_label)\n",
    "\n",
    "            valid_loss.append(loss)\n",
    "            step += 1\n",
    "            \n",
    "            \n",
    "        val_loss = sum(valid_loss)/step\n",
    "        print(f\"vaild {epoch} epoch 종료  평균 loss: {val_loss}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_save_path = './model/NLI/' +  'saved_model_epoch_' + str(epoch) + '.pt'\n",
    "            model.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koelectra",
   "language": "python",
   "name": "koelectra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
